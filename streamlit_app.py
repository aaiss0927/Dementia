import streamlit as st
import pandas as pd
import numpy as np
import datetime
import re
import joblib
import requests
import os
from openai import OpenAI
from streamlit_webrtc import webrtc_streamer, WebRtcMode, MediaStreamConstraints, AudioProcessorBase
from streamlit_drawable_canvas import st_canvas
import base64
import random # ë”ë¯¸ ëª¨ë¸ìš©
from PIL import Image
import io
from typing import List, Union
import av # media íŒŒì¼ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ (streamlit-webrtcì™€ í•¨ê»˜ ì‚¬ìš©ë¨)
from pydub import AudioSegment # pydubì„ ì‚¬ìš©í•˜ì—¬ audio segment ì²˜ë¦¬ ë° WAV ì €ì¥
import threading # ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ìŠ¤ë ˆë”©

import warnings
warnings.filterwarnings('ignore')

# --- 0. ì „ì—­ ì„¤ì • ë° API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
try:
    client = OpenAI(api_key=st.secrets["openai_api_key"])
except Exception:
    client = None
    st.error("âš ï¸ OpenAI API í‚¤ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. '.streamlit/secrets.toml' íŒŒì¼ì„ í™•ì¸í•˜ê±°ë‚˜ ë°°í¬ í™˜ê²½ì—ì„œ ì„¤ì •í•´ì£¼ì„¸ìš”.")

# ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ ê²½ë¡œ
MODEL_FILE = 'final_model.joblib'
SCALER_FILE = 'final_scaler.joblib'

# --- 1. ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”© ---
@st.cache_resource
def load_model_and_scaler():
    """ì‹¤ì œ í•™ìŠµëœ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        model = joblib.load(MODEL_FILE)
        scaler = joblib.load(SCALER_FILE)
        st.success("âœ… ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ.")
        return model, scaler
    except FileNotFoundError:
        st.error("âš ï¸ 'final_model.joblib' ë˜ëŠ” 'final_scaler.joblib' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë”ë¯¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        # ë”ë¯¸ ëª¨ë¸ (ì‹¤ì œ ë°°í¬ ì‹œ ì´ ë¶€ë¶„ ì‚­ì œ)
        class DummyModel:
            def predict(self, X): 
                # ë°ì´í„°ì…‹ì˜ íƒ€ê²Ÿì´ Dem/CN ì´ì—ˆë‹¤ê³  ê°€ì •
                return np.array([random.choice(['Dem', 'CN'])]) 
            def predict_proba(self, X): return np.array([[0.5, 0.5]])
        class DummyScaler:
            def transform(self, X): return X
        return DummyModel(), DummyScaler()

model, scaler = load_model_and_scaler()

# --- 2. ì±„ì  ë¡œì§ í•¨ìˆ˜ ---

def get_current_korean_season(month):
    """í˜„ì¬ ì›”ì— ë”°ë¥¸ í•œêµ­ ê³„ì ˆ ë°˜í™˜ (ê°„ì†Œí™”)"""
    if month in [3, 4, 5]: return "ë´„"
    elif month in [6, 7, 8]: return "ì—¬ë¦„"
    elif month in [9, 10, 11]: return "ê°€ì„"
    else: return "ê²¨ìš¸"

def score_time_date(q_num, user_input, current_dt):
    """Q01 ~ Q05 ì‹œê°„/ë‚ ì§œ ì§€ë‚¨ë ¥ ì±„ì  ë¡œì§"""
    score = 0
    if q_num == 1: # ì—°ë„
        if user_input == str(current_dt.year): score = 1
    elif q_num == 2: # ê³„ì ˆ
        if user_input == get_current_korean_season(current_dt.month): score = 1
    elif q_num == 3: # ì¼ (ë‚ ì§œ)
        if user_input == str(current_dt.day): score = 1
    elif q_num == 4: # ìš”ì¼
        korean_day = ["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"][current_dt.weekday()]
        if user_input == korean_day: score = 1
    elif q_num == 5: # ì›”
        if user_input == str(current_dt.month): score = 1
    return score

@st.cache_data
def get_user_location():
    """ê³µê°œ IP ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ì ìœ„ì¹˜ (êµ­ê°€, ë„ì‹œ)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        ip_response = requests.get('https://api.ipify.org?format=json')
        public_ip = ip_response.json()['ip']
        location_response = requests.get(f'https://ipapi.co/{public_ip}/json/')
        data = location_response.json()
        country = "ëŒ€í•œë¯¼êµ­" if data.get('country_code') == 'KR' else data.get('country_name', 'ì•Œ ìˆ˜ ì—†ìŒ')
        city = data.get('city', 'ì•Œ ìˆ˜ ì—†ìŒ')
        return country, city
    except Exception:
        return "ì•Œ ìˆ˜ ì—†ìŒ", "ì•Œ ìˆ˜ ì—†ìŒ"

def score_stt_response(audio_file_path, target_keywords=None, model_to_use="whisper-1"):
    """
    OpenAI Whisper APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ íŒŒì¼ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ë¥¼ ì±„ì í•©ë‹ˆë‹¤.
    ì£¼ì˜: audio_file_pathëŠ” ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œì—¬ì•¼ í•©ë‹ˆë‹¤. (ì´ ì½”ë“œì—ì„œëŠ” ë”ë¯¸ ê²½ë¡œ ì‚¬ìš©)
    """
    if client is None:
        return 0, "STT API í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜"
    if not audio_file_path or not os.path.exists(audio_file_path):
        return 0, f"STT: ì˜¤ë””ì˜¤ íŒŒì¼ ì—†ìŒ ë˜ëŠ” ê²½ë¡œ ì˜¤ë¥˜: {audio_file_path}"
        
    try:
        with open(audio_file_path, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model=model_to_use, 
                file=audio_file, 
                language="ko"
            ).text.lower()
            
        if target_keywords: # Q18
            if any(keyword.lower() in transcript for keyword in target_keywords):
                score = 1
            else:
                score = 0
            return score, transcript
        else: # Q15: í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼ (ê°„ì†Œí™”)
            return 1, transcript
        
    except Exception as e:
        return 0, f"STT ì²˜ë¦¬ ì˜¤ë¥˜: {e}"

def score_llm_writing(writing_text):
    """OpenAI GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê¸€ì“°ê¸° ì ìˆ˜(0 ë˜ëŠ” 1)ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤. (Q19)"""
    if client is None or not writing_text:
        return 0
        
    system_prompt = ("ë‹¹ì‹ ì€ ì¸ì§€ ê¸°ëŠ¥ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ì ê¸€ì´ ì£¼ì–´ì§„ ì£¼ì œ('ë‚ ì”¨ ë˜ëŠ” ê¸°ë¶„')ì— ëŒ€í•´ 'í•˜ë‚˜ì˜ ì˜¨ì „í•œ ë¬¸ì¥'ì¸ì§€ íŒë‹¨í•˜ê³ , "
                    "ì˜¨ì „í•œ ë¬¸ì¥ì´ë©´ '1', ì•„ë‹ˆë©´ '0'ì„ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ ì¼ì ˆ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. (ì˜ˆ: 'ë¹„ê°€ ì˜¨ë‹¤'ëŠ” 1ì , 'ë¹„'ëŠ” 0ì )")
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ì‚¬ìš©ì ê¸€: '{writing_text}'"}
            ],
            temperature=0
        )
        score_match = re.search(r'[01]', response.choices[0].message.content)
        return int(score_match.group(0)) if score_match else 0
        
    except Exception:
        return 0

def score_registration_recall(user_input, target_words):
    """Q11, Q13 ë“±ë¡/ê¸°ì–µ íšŒìƒ ì±„ì  ë¡œì§ (ìˆœì„œ ë¬´ê´€, 3ê°œ ëª¨ë‘ í¬í•¨)"""
    user_words = set(re.findall(r'\b\w+\b', user_input.replace(',', ' ').lower()))
    target_set = set(target_words)
    return 1 if target_set.issubset(user_words) else 0

def score_drawing_similarity(original_image_url, user_drawing_data_url):
    """
    OpenAI Vision APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ê·¸ë¦¼ê³¼ ì‚¬ìš©ìê°€ ê·¸ë¦° ê·¸ë¦¼ì˜ ìœ ì‚¬ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. (Q17)
    """
    if client is None:
        return 0, "Vision API í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜"

    try:
        response = client.chat.completions.create(
            model="gpt-4o", # Vision ê¸°ëŠ¥ì´ ìˆëŠ” ëª¨ë¸
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "ì²« ë²ˆì§¸ ê·¸ë¦¼(ì›ë³¸)ì„ ë‘ ë²ˆì§¸ ê·¸ë¦¼(ì‚¬ìš©ìê°€ ê·¸ë¦° ê·¸ë¦¼)ì´ ì–¼ë§ˆë‚˜ ì˜ ëª¨ì‚¬í–ˆëŠ”ì§€ í‰ê°€í•˜ê³ , '1' (ë§¤ìš° ìœ ì‚¬) ë˜ëŠ” '0' (ìœ ì‚¬í•˜ì§€ ì•ŠìŒ)ìœ¼ë¡œë§Œ ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."},
                        {"type": "image_url", "image_url": {"url": original_image_url, "detail": "low"}},
                        {"type": "image_url", "image_url": {"url": user_drawing_data_url, "detail": "low"}},
                    ],
                }
            ],
            max_tokens=1, # 0 ë˜ëŠ” 1ë§Œ ë°›ë„ë¡ ì œí•œ
            temperature=0,
        )
        score_match = re.search(r'[01]', response.choices[0].message.content)
        return int(score_match.group(0)) if score_match else 0, "Vision API í‰ê°€ ì™„ë£Œ"
    except Exception as e:
        return 0, f"Vision API ì²˜ë¦¬ ì˜¤ë¥˜: {e}"


# Lock for thread safety during audio writing
audio_data_lock = threading.Lock()
# ë…¹ìŒ ë°ì´í„°ë¥¼ ì €ì¥í•  ë²„í¼
audio_buffers = {} 

class MyAudioProcessor(AudioProcessorBase):
    """
    WebRTCë¡œë¶€í„° ì˜¤ë””ì˜¤ í”„ë ˆì„ì„ ë°›ì•„ ë²„í¼ì— ì €ì¥í•˜ëŠ” í”„ë¡œì„¸ì„œ.
    """
    def __init__(self) -> None:
        self.samples = []
        self.lock = threading.Lock()

    def recv(self, frame: av.AudioFrame) -> Union[av.AudioFrame, None]:
        # ì˜¤ë””ì˜¤ í”„ë ˆì„ì„ samples ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        new_samples = frame.to_ndarray(format="s16", layout="mono")
        with self.lock:
            self.samples.append(new_samples)
        
        # í”„ë ˆì„ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì—¬ ì—°ê²° ìœ ì§€
        return frame

def st_webrtc_audio_recorder(key, component_label):
    """
    ì‹¤ì œ ì˜¤ë””ì˜¤ ë°ì´í„° ì €ì¥ì„ ìœ„í•œ WebRTC ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤.
    """
    audio_path_key = f"{key}_audio_path"
    
    # ì„¸ì…˜ ìƒíƒœê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
    if audio_path_key not in st.session_state:
        st.session_state[audio_path_key] = None

    # AudioProcessorë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¼ì„ ë°›ì•„ì˜µë‹ˆë‹¤.
    webrtc_ctx = webrtc_streamer(
        key=key,
        mode=WebRtcMode.SENDRECV, # ì˜¤ë””ì˜¤ë¥¼ ë°›ê³  ë³´ëƒ„
        media_stream_constraints=MediaStreamConstraints(video=False, audio=True),
        audio_processor_factory=MyAudioProcessor, # ì‚¬ìš©ì ì •ì˜ í”„ë¡œì„¸ì„œ ì‚¬ìš©
        rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
    )

    # ì˜¤ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥ ë²„íŠ¼
    if webrtc_ctx.audio_processor:
        processor = webrtc_ctx.audio_processor
        
        # ë…¹ìŒ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì €ì¥ ë²„íŠ¼ í™œì„±í™”
        if st.button(f"ì €ì¥ ë° ì±„ì  ({component_label})", key=f"{key}_save_btn"):
            st.warning("ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
            
            # 1. ë²„í¼ì—ì„œ ë°ì´í„° ì¶”ì¶œ
            with processor.lock:
                all_samples = np.concatenate(processor.samples, axis=0) if processor.samples else None
                processor.samples = [] # ë°ì´í„° ì¶”ì¶œ í›„ ë²„í¼ ì´ˆê¸°í™”
            
            if all_samples is None or all_samples.size == 0:
                st.error("ë…¹ìŒëœ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None

            # 2. NumPy ë°°ì—´ì„ pydub AudioSegmentë¡œ ë³€í™˜ ë° ì €ì¥
            try:
                # s16 (16ë¹„íŠ¸ ì •ìˆ˜)ë¡œ ê°€ì •
                audio_segment = AudioSegment(
                    all_samples.tobytes(), 
                    frame_rate=48000, # WebRTC ê¸°ë³¸ ìƒ˜í”Œë§ ì†ë„ (í™˜ê²½ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                    sample_width=all_samples.dtype.itemsize, 
                    channels=1
                )
                
                temp_audio_file = f"uploaded_{key}_{datetime.datetime.now().strftime('%M%S')}.wav"
                audio_segment.export(temp_audio_file, format="wav")
                
                st.session_state[audio_path_key] = temp_audio_file
                st.success(f"âœ… ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {temp_audio_file}")
                return temp_audio_file
                
            except Exception as e:
                st.error(f"ì˜¤ë””ì˜¤ ì €ì¥ ì‹¤íŒ¨: {e}")
                return None
    
    return st.session_state.get(audio_path_key)

# --- 3. Streamlit UI êµ¬ì„± ---

def app():
    st.set_page_config(page_title="MMSE ê°„ì´ ìê°€ ì§„ë‹¨ ì›¹ì‚¬ì´íŠ¸", layout="wide")
    st.title("ğŸ§  MMSE ê¸°ë°˜ ê°„ì´ ìê°€ ì§„ë‹¨ (ê³ ê¸‰ ì‹œë®¬ë ˆì´ì…˜)")
    st.markdown("---")
    
    current_dt = datetime.datetime.now()
    user_country, user_city = get_user_location()

    # Q11/Q13 ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
    target_words = {"ì‚¬ê³¼", "ì„¸íƒê¸°", "ì±…ìƒ"}
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'features' not in st.session_state:
        st.session_state.features = {k: 0 for k in ['Q01', 'Q02', 'Q03', 'Q04', 'Q05', 'Q06', 'Q07', 'Q08', 'Q09', 'Q10', 'Q11_1', 'Q11_2', 'Q11_3', 'Q12_1', 'Q12_2', 'Q12_3', 'Q12_4', 'Q12_5', 'Q13_1', 'Q13_2', 'Q13_3', 'Q14_1', 'Q14_2', 'Q15', 'Q16_1', 'Q16_2', 'Q16_3', 'Q17', 'Q18', 'Q19']}
        st.session_state.basic_info = {'SAMPLE_EMAIL': '', 'DIAG_SEQ': 1, 'MMSE_KIND': 2}
        st.session_state.q15_audio_path = None
        st.session_state.q18_audio_path = None
        st.session_state.q17_drawing_data_url = None

    with st.form(key='diagnosis_form'):
        
        # --- ê¸°ë³¸ ì •ë³´ ---
        st.header("ğŸ‘¤ ê¸°ë³¸ ì •ë³´")
        col1, col2 = st.columns(2)
        with col1:
            email = st.text_input("SAMPLE_EMAIL", key='email_input')
            st.session_state.basic_info['SAMPLE_EMAIL'] = email
        with col2:
            st.text_input("DIAG_SEQ", value="1", disabled=True)
            st.text_input("MMSE_KIND", value="2 (ê³ ì •)", disabled=True)
        st.markdown("---")
        
        # --- ì§€ë‚¨ë ¥ (ì‹œê°„ & ì¥ì†Œ) ---
        st.header("ğŸ•°ï¸ ì§€ë‚¨ë ¥")
        st.markdown("##### ì‹œê°„")
        q_cols = st.columns(5)
        q01 = q_cols[0].text_input("Q01: ì—°ë„", key='q01')
        st.session_state.features['Q01'] = score_time_date(1, q01, current_dt)
        q02 = q_cols[1].text_input("Q02: ê³„ì ˆ", key='q02')
        st.session_state.features['Q02'] = score_time_date(2, q02, current_dt)
        q03 = q_cols[2].text_input("Q03: ì¼", key='q03')
        st.session_state.features['Q03'] = score_time_date(3, q03, current_dt)
        q04 = q_cols[3].text_input("Q04: ìš”ì¼", key='q04')
        st.session_state.features['Q04'] = score_time_date(4, q04, current_dt)
        q05 = q_cols[4].text_input("Q05: ì›”", key='q05')
        st.session_state.features['Q05'] = score_time_date(5, q05, current_dt)
        
        st.markdown("##### ì¥ì†Œ")
        q_cols = st.columns(5)
        q06 = q_cols[0].text_input(f"Q06: êµ­ê°€ ({user_country})", key='q06')
        st.session_state.features['Q06'] = 1 if q06 == user_country else 0
        q07 = q_cols[1].text_input(f"Q07: ë„ì‹œ ({user_city})", key='q07')
        st.session_state.features['Q07'] = 1 if q07 == user_city else 0
        q08 = q_cols[2].text_input("Q08: ê±´ë¬¼ ìœ í˜•", key='q08')
        st.session_state.features['Q08'] = 1 if q08 else 0
        q09 = q_cols[3].text_input("Q09: ê±´ë¬¼ ì´ë¦„", key='q09')
        st.session_state.features['Q09'] = 1 if q09 else 0
        q10 = q_cols[4].text_input("Q10: ì¸µìˆ˜", key='q10')
        st.session_state.features['Q10'] = 1 if q10 else 0
        st.markdown("---")
        
        # --- Q11, Q13 ë“±ë¡/íšŒìƒ ---
        st.header("ğŸ ê¸°ì–µ ë“±ë¡ ë° íšŒìƒ")
        st.info(f"**ëª©í‘œ ë‹¨ì–´:** {', '.join(target_words)}")
        q11_input = st.text_input("Q11: ì„¸ ê°€ì§€ ë¬¼ê±´ ì´ë¦„ì„ ë”°ë¼ ë§ì”€í•´ë³´ì„¸ìš”.", key='q11_input')
        q11_score = score_registration_recall(q11_input, target_words)
        st.session_state.features['Q11_1'] = st.session_state.features['Q11_2'] = st.session_state.features['Q11_3'] = q11_score
        st.caption(f"Q11 ì±„ì  ê²°ê³¼: {q11_score}ì  (ì´ 1ì )")
        
        q13_input = st.text_input("Q13: ì•„ê¹Œ ì™¸ìš´ ë‹¨ì–´ ì„¸ ê°€ì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”.", key='q13_input')
        q13_score = score_registration_recall(q13_input, target_words)
        st.session_state.features['Q13_1'] = st.session_state.features['Q13_2'] = st.session_state.features['Q13_3'] = q13_score
        st.caption(f"Q13 ì±„ì  ê²°ê³¼: {q13_score}ì  (ì´ 1ì )")
        st.markdown("---")

        # --- Q12 ì£¼ì˜ ì§‘ì¤‘ ë° ê³„ì‚° ---
        st.header("ğŸ”¢ ì£¼ì˜ ì§‘ì¤‘ ë° ê³„ì‚° (Q12)")
        st.info("100ì—ì„œ 7ì„ ì—°ì†í•´ì„œ ë¹¼ë³´ì„¸ìš”. (5ë‹¨ê³„)")
        q12_answers = [93, 86, 79, 72, 65]
        q12_cols = st.columns(5)
        for i, answer in enumerate(q12_answers):
            with q12_cols[i]:
                q12_input = st.number_input(f"Q12_{i+1}: {100 - (i) * 7} - 7 = ?", key=f'q12_{i+1}', step=1, value=None, format="%d")
                score = 1 if q12_input and int(q12_input) == answer else 0
                st.session_state.features[f'Q12_{i+1}'] = score
                st.caption(f"({score}ì )")
        st.markdown("---")
        
        # --- Q14~Q19 ì–¸ì–´ ëŠ¥ë ¥ ë° ê¸°íƒ€ ---
        st.header("ğŸ—£ï¸ ì–¸ì–´ ë° ì‹¤í–‰ ëŠ¥ë ¥")
        
        # Q14 (ì´ë¦„ ëŒ€ê¸°)
        st.subheader("Q14: ì´ë¦„ ëŒ€ê¸°")
        st.image("https://i.imgur.com/vH0k1tM.png", caption="ì›ë³¸ ì´ë¯¸ì§€: ì‹œê³„", width=100) 
        q14_1 = st.text_input("Q14_1: ì´ê²ƒì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?", key='q14_1')
        st.session_state.features['Q14_1'] = 1 if 'ì‹œê³„' in q14_1 else 0
        
        st.image("https://i.imgur.com/kS5x0N9.png", caption="ì›ë³¸ ì´ë¯¸ì§€: ì—°í•„", width=100) 
        q14_2 = st.text_input("Q14_2: ì´ê²ƒì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?", key='q14_2')
        st.session_state.features['Q14_2'] = 1 if 'ì—°í•„' in q14_2 else 0

        # Q15 (ë”°ë¼ ë§í•˜ê¸° - STT êµ¬í˜„, í¼ ë‚´ë¶€)
        st.subheader("Q15: ë”°ë¼ ë§í•˜ê¸° (ë§ˆì´í¬)")
        st.markdown("'_ê°„ì¥ ê³µì¥ ê³µì¥ì¥_'ì„ ë§ˆì´í¬ì— ëŒ€ê³  ë§í•´ë³´ì„¸ìš”.")
        q15_audio_file_path = st_webrtc_audio_recorder("Q15", "Q15 ë”°ë¼ ë§í•˜ê¸°")
        if q15_audio_file_path:
            st.session_state.q15_audio_path = q15_audio_file_path
        st.markdown("---")
            
        # Q16 (3ë‹¨ê³„ ëª…ë ¹)
        st.subheader("Q16: 3ë‹¨ê³„ ëª…ë ¹ ìˆ˜í–‰")
        st.text("ì¢…ì´ë¥¼ ë“œë¦¬ë©´, ì¢…ì´ë¥¼ ë’¤ì§‘ì€ ë‹¤ìŒ, ë°˜ìœ¼ë¡œ ì ‘ì–´ì„œ ì €ì—ê²Œ ì£¼ì„¸ìš”.")
        q16_1 = st.checkbox("Q16_1: ì¢…ì´ë¥¼ ë’¤ì§‘ì—ˆìŠµë‹ˆë‹¤.", key='q16_1')
        q16_2 = st.checkbox("Q16_2: ë°˜ìœ¼ë¡œ ì ‘ì—ˆìŠµë‹ˆë‹¤.", key='q16_2')
        q16_3 = st.checkbox("Q16_3: ì €ì—ê²Œ ì£¼ì—ˆìŠµë‹ˆë‹¤.", key='q16_3')
        st.session_state.features['Q16_1'] = 1 if q16_1 else 0
        st.session_state.features['Q16_2'] = 1 if q16_2 else 0
        st.session_state.features['Q16_3'] = 1 if q16_3 else 0

        # Q17 (ë”°ë¼ ê·¸ë¦¬ê¸° - Canvas + OpenAI Vision API)
        st.subheader("Q17: ë”°ë¼ ê·¸ë¦¬ê¸°")
        q17_original_image_url = "https://i.imgur.com/gK9p5Fz.png" # ì›ë³¸ ê·¸ë¦¼ URL
        st.image(q17_original_image_url, caption="ì›ë³¸ ê·¸ë¦¼: ì˜¤ê°í˜•ê³¼ ì‚¬ê°í˜•", width=150)
        
        canvas_result = st_canvas(
            fill_color="rgba(255, 165, 0, 0)", # ì±„ìš°ê¸° ìƒ‰ ì—†ìŒ
            stroke_width=3, # íœ ë‘ê»˜ ì¡°ì ˆ (ê¸°ë³¸ê°’ 3ìœ¼ë¡œ ì„¤ì •)
            stroke_color="#000000", # ê²€ì€ìƒ‰ íœ
            background_image=None,
            width=250,
            height=250,
            drawing_mode="freedraw",
            key="canvas",
        )
        
        # ê·¸ë¦° ê·¸ë¦¼ ë°ì´í„°ë¥¼ base64 Data URLë¡œ ì €ì¥ (ì œì¶œ ì‹œ Vision API í˜¸ì¶œ)
        if canvas_result.image_data is not None:
            # ğŸ”¥ğŸ”¥ğŸ”¥ ìˆ˜ì •ëœ ë¶€ë¶„: NumPy ë°°ì—´ì„ PIL Image ê°ì²´ë¡œ ë³€í™˜ ğŸ”¥ğŸ”¥ğŸ”¥
            image_array = canvas_result.image_data # NumPy ndarray
            
            # RGBA ë°°ì—´ì„ RGBë¡œ ë³€í™˜ (PIL Image.fromarrayëŠ” RGBAë„ ì²˜ë¦¬ ê°€ëŠ¥)
            pil_image = Image.fromarray(image_array.astype('uint8'), 'RGBA') 
            
            # 2. PIL Image ê°ì²´ì˜ save ë©”ì„œë“œ ì‚¬ìš©
            buffered = io.BytesIO()
            pil_image.save(buffered, format="PNG") # PIL Image ê°ì²´ëŠ” save() ë©”ì„œë“œë¥¼ ê°€ì§
            
            img_str = base64.b64encode(buffered.getvalue()).decode()
            st.session_state.q17_drawing_data_url = f"data:image/png;base64,{img_str}"
        else:
            st.session_state.q17_drawing_data_url = None
        st.markdown("---")

        # Q18 (ì½ê³  ìˆ˜í–‰ - STT êµ¬í˜„, í¼ ë‚´ë¶€)
        st.subheader("Q18: ë¬¸ì¥ ì½ê³  ìˆ˜í–‰ (ë§ˆì´í¬)")
        st.markdown("'_ëˆˆì„ ê°ìœ¼ì„¸ìš”_' ë¬¸ì¥ì„ ë§ˆì´í¬ì— ëŒ€ê³  ì½ì€ í›„, ëˆˆì„ ê°ëŠ” ì‹œëŠ‰ì„ í•´ë³´ì„¸ìš”.")
        q18_audio_file_path = st_webrtc_audio_recorder("Q18", "Q18 ì½ê³  ìˆ˜í–‰")
        if q18_audio_file_path:
            st.session_state.q18_audio_path = q18_audio_file_path
        st.markdown("---")

        # Q19 (ê¸€ì“°ê¸° - LLM ì±„ì )
        st.subheader("Q19: ë¬¸ì¥ ë§Œë“¤ê¸°")
        st.text("ì—¬ê¸°ì— ë‚ ì”¨(ë˜ëŠ” ê¸°ë¶„)ì— ëŒ€í•´ì„œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¨ ë³´ì„¸ìš”.")
        q19_text = st.text_area("Q19: ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”.", key='q19')
        st.markdown("---")
        
        submit_button = st.form_submit_button(label='ìê°€ ì§„ë‹¨ ê²°ê³¼ í™•ì¸ ë° ì˜ˆì¸¡')

    # --- 4. ì œì¶œ ë° ì˜ˆì¸¡ ---
    if submit_button:
        # 1. STT ìµœì¢… ì±„ì  (Q15, Q18)
        q15_score, q15_transcript = score_stt_response(st.session_state.q15_audio_path, target_keywords=None)
        st.session_state.features['Q15'] = q15_score
        
        q18_score, q18_transcript = score_stt_response(st.session_state.q18_audio_path, target_keywords=["ëˆˆì„ ê°ìœ¼ì„¸ìš”"]) 
        st.session_state.features['Q18'] = q18_score
        
        # 2. Q17 Vision API ì±„ì 
        q17_score, q17_vision_status = 0, "ê·¸ë¦° ê·¸ë¦¼ ì—†ìŒ"
        if st.session_state.q17_drawing_data_url:
            q17_score, q17_vision_status = score_drawing_similarity(q17_original_image_url, st.session_state.q17_drawing_data_url)
        st.session_state.features['Q17'] = q17_score
        
        # 3. LLM ìµœì¢… ì±„ì  (Q19)
        q19_score = score_llm_writing(q19_text)
        st.session_state.features['Q19'] = q19_score
        
        # 4. ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
        features_for_model = {**st.session_state.basic_info, **st.session_state.features}
        features_for_model.pop('SAMPLE_EMAIL')
        
        feature_order = [
            'DIAG_SEQ', 'MMSE_KIND', 
            'Q01', 'Q02', 'Q03', 'Q04', 'Q05', 'Q06', 'Q07', 'Q08', 'Q09', 'Q10',
            'Q11_1', 'Q11_2', 'Q11_3', 
            'Q12_1', 'Q12_2', 'Q12_3', 'Q12_4', 'Q12_5', 
            'Q13_1', 'Q13_2', 'Q13_3', 
            'Q14_1', 'Q14_2', 
            'Q15', 'Q16_1', 'Q16_2', 'Q16_3', 'Q17', 'Q18', 'Q19'
        ]
        
        input_data = {k: features_for_model[k] for k in feature_order}
        input_df = pd.DataFrame([input_data])

        # 5. ì˜ˆì¸¡ ìˆ˜í–‰
        if model is not None and scaler is not None:
            input_scaled = scaler.transform(input_df)
            prediction = model.predict(input_scaled)
            result_text = "Dem (ì¹˜ë§¤/ìœ„í—˜êµ°)" if prediction[0] == 'Dem' else "CN (ì •ìƒ)" 
        else:
            result_text = "ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨"

        st.subheader("âœ… ìê°€ ì§„ë‹¨ ê²°ê³¼")
        if "Dem" in result_text:
            st.error(f"**ì§„ë‹¨ ê²°ê³¼:** {result_text}")
            st.warning("âš ï¸ ì´ ê²°ê³¼ëŠ” ì¹˜ë§¤ ìœ„í—˜êµ°(MCI/Dem)ìœ¼ë¡œ ë¶„ë¥˜ëœ ì‹œë®¬ë ˆì´ì…˜ì…ë‹ˆë‹¤. ì „ë¬¸ì˜ì˜ ì§„ë£Œë¥¼ ë°˜ë“œì‹œ ë°›ìœ¼ì‹­ì‹œì˜¤.")
        elif "CN" in result_text:
            st.success(f"**ì§„ë‹¨ ê²°ê³¼:** {result_text}")
            st.info("ì´ ê²°ê³¼ëŠ” ê°„ì´ ì§„ë‹¨ì´ë©°, ì •ê¸°ì ì¸ ê²€ì§„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            st.error(result_text)
        
        st.subheader("ğŸ“Š ì±„ì  ê²°ê³¼ ìš”ì•½")
        # MMSE ì´ì  (DIAG_SEQ, MMSE_KINDë¥¼ ì œì™¸í•œ Q ì ìˆ˜ í•©ì‚°)
        total_score = input_df.iloc[0].drop(['DIAG_SEQ', 'MMSE_KIND']).sum()
        st.metric(label="ì´ ì ìˆ˜ (Max 26ì )", value=f"{total_score}ì ")
        
        st.caption(f"Q15 (ë”°ë¼ ë§í•˜ê¸°) ì±„ì : {q15_score}ì  (STT ì „ì‚¬: {q15_transcript[:50]}...)")
        st.caption(f"Q18 (ì½ê³  ìˆ˜í–‰) ì±„ì : {q18_score}ì  (STT ì „ì‚¬: {q18_transcript[:50]}...)")
        st.caption(f"Q17 (ë”°ë¼ ê·¸ë¦¬ê¸°) ì±„ì : {q17_score}ì  ({q17_vision_status})")
        st.caption(f"Q19 (ê¸€ì“°ê¸°) ì±„ì : {q19_score}ì  (LLM íŒì •)")
        
        st.dataframe(input_df.T.rename(columns={0: 'ì…ë ¥ í”¼ì²˜ ê°’'}), use_container_width=True)


if __name__ == "__main__":
    app()